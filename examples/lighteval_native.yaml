##lighteval
model: google/gemma-3-4b-pt #Qwen/Qwen2.5-0.5B-Instruct
chat_template: false
precision: auto
backend: lighteval

tasks: tasks/native_tasks.txt
custom_tasks: frameworks/lighteval/community_tasks/3lm_eval.py
OUTPUT_DIR: "results"   
# max_samples: 5
## accelerate
DP: 8  ## specify data parallelism  (copies of the model to run in parallel)
MP: 1  ## how many gpus each copy of the model will be using